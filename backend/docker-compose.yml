version: '3.8'

services:
  backend:
    container_name: ecommerce-backend
    build: .
    ports:
      - "4000:4000"
    volumes:
      - .:/app
      - /app/node_modules
    env_file:
      - .env
    depends_on:
      - mongo
      - kafka
      - zookeeper
    networks:
      - ecommerce-network

  mongo:
    image: mongo
    container_name: mongodb
    ports:
      - "27017:27017"
    volumes:
      - mongo_data:/data/db
    networks:
      - ecommerce-network

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    networks:
      - ecommerce-network

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"  # để máy thật (Postman, debug tools) có thể kết nối nếu cần
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: |
        PLAINTEXT://kafka:9092,
        PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: |
        PLAINTEXT:PLAINTEXT,
        PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
      - ecommerce-network

  spark:
    image: bitnami/spark:latest
    container_name: spark-processor
    depends_on:
      - kafka
      - mongo
    environment:
      - SPARK_MODE=driver
    volumes:
      - ./kafka:/spark # Mount the local directory containing the Spark job
    command:
      - "/opt/bitnami/spark/bin/spark-submit"
      - "--packages"
      - "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,org.mongodb.spark:mongo-spark-connector_2.12:3.0.1"
      - "/spark/spark_processor.py" # Path to your Spark job
    networks:
      - ecommerce-network


volumes:
  mongo_data:

networks:
  ecommerce-network:
    driver: bridge


